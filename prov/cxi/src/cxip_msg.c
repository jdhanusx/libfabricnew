/*
 * SPDX-License-Identifier: GPL-2.0
 *
 * Copyright (c) 2018 Cray Inc. All rights reserved.
 */

#include "config.h"

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/time.h>
#include <sys/types.h>

#include <ofi_list.h>
#include <ofi.h>

#include "cxip.h"

#define CXIP_LOG_DBG(...) _CXIP_LOG_DBG(FI_LOG_EP_DATA, __VA_ARGS__)
#define CXIP_LOG_ERROR(...) _CXIP_LOG_ERROR(FI_LOG_EP_DATA, __VA_ARGS__)

/* Caller must hold rxc->lock */
static struct cxip_ux_send *
match_ux_send(struct cxip_rx_ctx *rxc, const union c_event *event)
{
	struct cxip_ux_send *ux_send;

	/* Look for a previously received unexpected Put event with matching
	 * start pointer.
	 *
	 * TODO this assumes all overflow buffers use the same AC so all start
	 * pointers are unique.
	 */
	dlist_foreach_container(&rxc->ux_sends, struct cxip_ux_send, ux_send,
				list) {
		if (ux_send->start == event->tgt_long.start) {
			dlist_remove(&ux_send->list);
			return ux_send;
		}
	}

	return NULL;
}

/* Caller must hold rxc->lock */
static struct cxip_req *
match_ux_recv(struct cxip_rx_ctx *rxc, const union c_event *event)
{
	struct cxip_req *req;

	/* Look for a previously completed request which was matched to an
	 * overflow buffer that has a matching start pointer.
	 *
	 * TODO this assumes all overflow buffers use the same AC so all start
	 * pointers are unique.
	 */
	dlist_foreach_container(&rxc->ux_recvs, struct cxip_req, req, list) {
		if (req->recv.start == event->tgt_long.start) {
			dlist_remove(&req->list);
			return req;
		}
	}

	return NULL;
}

static void report_recv_completion(struct cxip_req *req)
{
	int ret;
	int truncated;
	int err;

	truncated = req->recv.rlength - req->recv.mlength;
	if (req->recv.rc == C_RC_OK && !truncated) {
		req->data_len = req->recv.mlength;

		ret = req->cq->report_completion(req->cq, FI_ADDR_UNSPEC, req);
		if (ret != req->cq->cq_entry_size)
			CXIP_LOG_ERROR("Failed to report completion: %d\n",
				       ret);
	} else {
		err = truncated ? FI_EMSGSIZE : FI_EIO;

		ret = cxip_cq_report_error(req->cq, req, truncated, err,
					   req->recv.rc, NULL, 0);
		if (ret != FI_SUCCESS)
			CXIP_LOG_ERROR("Failed to report error: %d\n", ret);
	}
}

static void oflow_buf_free(struct cxip_oflow_buf *oflow_buf)
{
	struct cxip_if *dev_if = oflow_buf->rxc->domain->dev_if;

	cxil_unmap(dev_if->if_lni, &oflow_buf->md);
	free(oflow_buf->buf);
	free(oflow_buf);
}

static void oflow_buf_get(struct cxip_oflow_buf *oflow_buf)
{
	ofi_atomic_inc32(&oflow_buf->ref);
}

static void oflow_buf_put(struct cxip_oflow_buf *oflow_buf)
{
	if (!ofi_atomic_dec32(&oflow_buf->ref)) {
		oflow_buf_free(oflow_buf);
	}
}

/* Process an overflow buffer event.
 *
 * We can expect Link, Unlink and Put events from the overflow buffer.
 *
 * A Link event arrives when the append has been completed.  If successful Link
 * events are suppressed, it can be assumed that the event contains an error
 * and the append failed.
 *
 * An Unlink event is expected when the overflow buffer space is exhausted.
 * Overflow buffers are configured to use locally managed LEs.  When enough
 * Puts have matched in an overflow buffer, consuming its space, the NIC will
 * automatically unlink the LE.  An automatic Unlink event will be generated
 * before the final Put which caused space to be exhausted.
 *
 * An Unlink may also be generated by an Unlink command.  In this case, the
 * auto_unlinked field in the event will be zero.  In this case, free the
 * request immediately.
 *
 * A Put event will be generated for each Put that matches the overflow buffer
 * LE.  The Put event indicates that data is in the overflow buffer to be
 * copied into a user buffer.  This event must be correlated to a Put_Overflow
 * event from a user buffer LE.  The Put_Overflow event may arrive before or
 * after the Put event.
 *
 * When each Put event arrives, check for the existence of a previously posted
 * receive buffer which generated a matching Put_Overflow event.  If such a
 * buffer exists, copy data from the overflow buffer to the user buffer.
 * Otherwise, store a record of the Put event for matching once a user posts a
 * new buffer that matches.
 *
 * If data will remain in the overflow buffer, take a reference to it to
 * prevent it from being freed.  If a sequence of Unlink-Put events is
 * detected, drop a reference to the overflow buffer so it is automatically
 * freed once all user data is copied out.
 */
static void cxip_oflow_cb(struct cxip_req *req, const union c_event *event)
{
	struct cxip_rx_ctx *rxc = req->oflow.rxc;
	struct cxip_req *ux_recv;
	struct cxip_ux_send *ux_send;
	struct cxip_oflow_buf *oflow_buf;
	void *oflow_va;

	oflow_buf = req->oflow.oflow_buf;

	CXIP_LOG_DBG("got event: %d\n", event->hdr.event_type);

	/* Netsim is currently giving events in the order: LINK-UNLINK-PUT.
	 * Assume this order is guaranteed for now.
	 */

	if (event->hdr.event_type == C_EVENT_LINK) {
		/* TODO Handle append errors. */
		return;
	}

	if (event->hdr.event_type == C_EVENT_UNLINK) {
		ofi_atomic_dec32(&rxc->oflow_buf_cnt);

		/* Check if this LE was Unlinked explicitly or
		 * automatically unlinked due to buffer exhaustion.
		 */
		if (!event->tgt_long.auto_unlinked) {
			cxip_cq_req_free(req);
			return;
		}

		/* Mark the overflow buffer exhausted.  One more Put event is
		 * expected.  When the event for the Put which exhausted
		 * resources arrives, drop a reference to the overflow buffer.
		 */
		oflow_buf->exhausted = 1;

		/* Refill overflow buffers */
		cxip_rxc_oflow_replenish(rxc);
		return;
	}

	if (event->hdr.event_type != C_EVENT_PUT) {
		CXIP_LOG_ERROR("Unexpected event type: %d\n",
			       event->hdr.event_type);
		return;
	}

	fastlock_acquire(&rxc->lock);

	/* Check for a previously received PUT_OVERFLOW event */
	ux_recv = match_ux_recv(rxc, event);
	if (!ux_recv) {
		/* A PUT_OVERFLOW event is pending.  Store a record of this
		 * unexepected Put event for lookup when the event arrives.
		 */

		/* TODO make fast allocator for ux_sends */
		ux_send = malloc(sizeof(struct cxip_ux_send));
		if (!ux_send) {
			CXIP_LOG_ERROR("Failed to malloc ux_send\n");
			abort();
		}

		ux_send->oflow_buf = oflow_buf;
		ux_send->start = event->tgt_long.start;
		ux_send->length = event->tgt_long.mlength;

		CXIP_LOG_DBG("Queued ux_send, data: 0x%lx\n", ux_send->start);

		/* Prevent the overflow buffer from being freed until the user
		 * has copied out data.
		 */
		oflow_buf_get(oflow_buf);

		dlist_insert_tail(&ux_send->list,
				  &rxc->ux_sends);

		fastlock_release(&rxc->lock);
	} else {
		/* A matching PUT_OVERFLOW event arrived earlier.  Data is
		 * waiting in the overflow buffer.
		 */

		fastlock_release(&rxc->lock);

		CXIP_LOG_DBG("Matched ux_recv, data: 0x%lx\n",
			     ux_recv->recv.start);

		/* Copy data from the overflow buffer */
		oflow_va = (void *)CXI_IOVA_TO_VA(&oflow_buf->md,
						  ux_recv->recv.start);

		if (event->tgt_long.mlength > req->recv.rlength) {
			req->recv.mlength = req->recv.rlength;
			req->recv.rlength = event->tgt_long.mlength;
		} else {
			req->recv.rlength = req->recv.mlength =
					event->tgt_long.mlength;
		}

		memcpy(ux_recv->recv.recv_buf, oflow_va, ux_recv->recv.mlength);

		report_recv_completion(ux_recv);

		/* Free the matched user buffer request */
		cxip_cq_req_free(ux_recv);
	}

	if (oflow_buf->exhausted) {
		CXIP_LOG_DBG("Oflow buf exhausted, buf: %p\n", oflow_buf->buf);

		/* This is the last event to the overflow buffer, drop a
		 * reference to it.  The buffer will be freed once all user
		 * data is copied out.
		 */
		oflow_buf_put(oflow_buf);

		/* No further events are expected, free the overflow buffer
		 * request immediately.
		 */
		cxip_cq_req_free(req);
	}
}

/* Append a new overflow buffer to an RX Context. */
static int oflow_buf_add(struct cxip_rx_ctx *rxc)
{
	struct cxip_domain *dom;
	int ret;
	struct cxip_oflow_buf *oflow_buf;
	struct cxip_req *req;
	union c_cmdu cmd = {};

	dom = rxc->domain;

	/* Create an overflow buffer structure */
	oflow_buf = calloc(1, sizeof(*oflow_buf));
	if (!oflow_buf) {
		CXIP_LOG_ERROR("Unable to allocate oflow buffer structure\n");
		return -FI_ENOMEM;
	}

	/* Allocate overflow data buffer */
	oflow_buf->buf = calloc(1, rxc->oflow_buf_size);
	if (!oflow_buf->buf) {
		CXIP_LOG_ERROR("Unable to allocate oflow buffer\n");
		ret = -FI_ENOMEM;
		goto free_oflow;
	}

	/* Map overflow data buffer */
	ret = cxil_map(dom->dev_if->if_lni, (void *)oflow_buf->buf,
		       rxc->oflow_buf_size,
		       CXI_MAP_PIN | CXI_MAP_NTA | CXI_MAP_WRITE,
		       &oflow_buf->md);
	if (ret) {
		CXIP_LOG_DBG("Failed to map oflow buffer: %d\n", ret);
		goto free_buf;
	}

	/* Populate request */
	req = cxip_cq_req_alloc(rxc->comp.recv_cq, 1);
	if (!req) {
		CXIP_LOG_DBG("Failed to allocate request\n");
		ret = -FI_ENOMEM;
		goto unmap;
	}

	req->cb = cxip_oflow_cb;
	req->oflow.rxc = rxc;
	req->oflow.oflow_buf = oflow_buf;

	/* Build Append command descriptor */
	cmd.command.opcode      = C_CMD_TGT_APPEND;
	cmd.target.ptl_list     = C_PTL_LIST_OVERFLOW;
	cmd.target.ptlte_index  = rxc->rx_pte->pte->ptn;
	cmd.target.op_put       = 1;
	cmd.target.buffer_id    = req->req_id;
	cmd.target.lac          = oflow_buf->md.lac;
	cmd.target.start        = CXI_VA_TO_IOVA(&oflow_buf->md,
						 oflow_buf->buf);
	cmd.target.length       = rxc->oflow_buf_size;
	cmd.target.ignore_bits  = (-1ULL); /* Match everything */
	cmd.target.manage_local = 1;
	cmd.target.no_truncate  = 1;
	cmd.target.min_free     = rxc->eager_threshold;

	fastlock_acquire(&rxc->lock);

	/* Multiple callers can race to allocate overflow buffers */
	if (ofi_atomic_get32(&rxc->oflow_buf_cnt) >= rxc->oflow_bufs_max) {
		ret = FI_SUCCESS;
		goto unlock;
	}

	/* Issue Append command */
	ret = cxi_cq_emit_target(rxc->rx_cmdq, &cmd);
	if (ret) {
		CXIP_LOG_DBG("Failed to write Append command: %d\n", ret);

		/* Return error according to Domain Resource Management */
		ret = -FI_EAGAIN;
		goto unlock;
	}

	cxi_cq_ring(rxc->rx_cmdq);

	/* Initialize oflow_buf structure */
	dlist_insert_tail(&oflow_buf->list, &rxc->oflow_bufs);
	oflow_buf->rxc = rxc;
	ofi_atomic_initialize32(&oflow_buf->ref, 1);
	oflow_buf->exhausted = 0;
	oflow_buf->buffer_id = req->req_id;

	ofi_atomic_inc32(&rxc->oflow_buf_cnt);

	/* TODO take reference on EP or context for the outstanding request */
	fastlock_release(&rxc->lock);

	return FI_SUCCESS;

unlock:
	fastlock_release(&rxc->lock);
	cxip_cq_req_free(req);
unmap:
	cxil_unmap(dom->dev_if->if_lni, &oflow_buf->md);
free_buf:
	free(oflow_buf->buf);
free_oflow:
	free(oflow_buf);

	return ret;
}

/* Replenish RX Context overflow buffers. */
void cxip_rxc_oflow_replenish(struct cxip_rx_ctx *rxc)
{
	int ret;

	while (ofi_atomic_get32(&rxc->oflow_buf_cnt) < rxc->oflow_bufs_max) {
		ret = oflow_buf_add(rxc);
		if (ret != FI_SUCCESS) {
			CXIP_LOG_ERROR("Failed to append oflow buffer: %d\n",
				       ret);
			break;
		}
	}
}

/* Free RX Context overflow buffers.
 *
 * The RXC must be disabled with no outstanding posted receives.  Adding new
 * posted receives that could match the overflow buffers while cleanup is in
 * progress will cause issues.  Also, with the RXC lock held, processing
 * messages on the context may cause a deadlock.
 *
 * Caller must hold rxc->lock.
 */
void cxip_rxc_oflow_cleanup(struct cxip_rx_ctx *rxc)
{
	int ret;
	union c_cmdu cmd = {};
	struct cxip_oflow_buf *oflow_buf;
	struct cxip_ux_send *ux_send;
	struct dlist_entry *itmp;
	struct dlist_entry *otmp;

	cmd.command.opcode = C_CMD_TGT_UNLINK;
	cmd.target.ptl_list = C_PTL_LIST_OVERFLOW;
	cmd.target.ptlte_index  = rxc->rx_pte->pte->ptn;

	/* Manually unlink each overflow buffer */
	dlist_foreach_container(&rxc->oflow_bufs, struct cxip_oflow_buf,
				oflow_buf, list) {
		cmd.target.buffer_id = oflow_buf->buffer_id;

		ret = cxi_cq_emit_target(rxc->rx_cmdq, &cmd);
		if (ret) {
			/* TODO handle insufficient CMDQ space */
			CXIP_LOG_ERROR("Failed to enqueue command: %d\n", ret);
		}
	}

	cxi_cq_ring(rxc->rx_cmdq);

	/* Wait for all overflow buffers to be unlinked */
	do {
		sched_yield();
		cxip_cq_progress(rxc->comp.recv_cq);
	} while (ofi_atomic_get32(&rxc->oflow_buf_cnt));

	/* Clean up overflow buffers */
	dlist_foreach_container_safe(&rxc->oflow_bufs, struct cxip_oflow_buf,
				     oflow_buf, list, otmp) {

		dlist_foreach_container_safe(&rxc->ux_sends,
					     struct cxip_ux_send, ux_send,
					     list, itmp) {
			if (ux_send->oflow_buf == oflow_buf) {
				dlist_remove(&ux_send->list);
				free(ux_send);
			}
		}

		dlist_remove(&oflow_buf->list);
		oflow_buf_free(oflow_buf);
	}
}

/* Process a posted receive buffer event.
 *
 * We can expect Link, Unlink, Put and Put_Overflow events from a posted
 * receive buffer.
 *
 * When a pre-posted receive matches an incoming send, netsim generates events
 * in the order: Link-Unlink-Put.  This order is guaranteed.  In this case, the
 * Put event indicates that the LE was matched in hardware and the operation is
 * complete.
 *
 * TODO Cassini allows successful Link and Unlink events to be suppressed.
 * Configure LEs to suppress these events.  In that case, a Link or Unlink
 * event would indicate a transaction failure.  Handle those errors.
 *
 * When a receive matches an unexpected header during the append, netsim
 * generates a Put_Overflow event.  There are no Link events associated with
 * the user buffer in this case.  The Put_Overflow event must be correlated
 * with a Put event generated from an overflow buffer.  The Put event may be
 * generated before or after the Put_Overflow event.
 *
 * When the Put_Overflow event arrives, check for the existence of a previously
 * received, matching Put event from an overflow buffer.  If such an event
 * exists, data may be copied from the overflow buffer to the user buffer and
 * the operation is completed.  If a matching event is not found, store a
 * record of the posted user buffer to be matched when the forthcoming Put
 * event arrives.
 */
static void cxip_trecv_cb(struct cxip_req *req, const union c_event *event)
{
	int ret;
	struct cxip_rx_ctx *rxc = req->recv.rxc;
	struct cxip_ux_send *ux_send;
	struct cxip_oflow_buf *oflow_buf;
	void *oflow_va;

	CXIP_LOG_DBG("got event: %d\n", event->hdr.event_type);

	if (event->hdr.event_type == C_EVENT_LINK) {
		/* TODO Handle append errors. */
		return;
	}

	if (event->hdr.event_type == C_EVENT_UNLINK)
		return;

	req->recv.rc = event->tgt_long.return_code;
	req->tag = event->tgt_long.match_bits;

	ret = cxil_unmap(req->cq->domain->dev_if->if_lni, &req->recv.recv_md);
	if (ret != FI_SUCCESS)
		CXIP_LOG_ERROR("Failed to free MD: %d\n", ret);

	if (event->hdr.event_type == C_EVENT_PUT_OVERFLOW) {
		/* We matched an unexpected header */

		fastlock_acquire(&rxc->lock);

		/* Check for a previously received unexpected Put event */
		ux_send = match_ux_send(rxc, event);
		if (!ux_send) {
			/* An unexpected Put event is pending.  Link this
			 * request to the pending list for lookup when the
			 * event arrives.
			 */

			/* Store start address to use for matching against
			 * future events.
			 */
			req->recv.start = event->tgt_long.start;

			dlist_insert_tail(&req->list, &rxc->ux_recvs);

			CXIP_LOG_DBG("Queued recv req, data: 0x%lx\n",
				     req->recv.start);

			fastlock_release(&rxc->lock);

			return;
		}

		fastlock_release(&rxc->lock);

		CXIP_LOG_DBG("Matched ux_send, data: 0x%lx\n", ux_send->start);

		/* A matching, unexpected Put event arrived earlier.  Data is
		 * waiting in the overflow buffer.
		 */
		oflow_buf = ux_send->oflow_buf;

		/* Copy data from the overflow buffer */
		oflow_va = (void *)CXI_IOVA_TO_VA(&oflow_buf->md,
						  event->tgt_long.start);

		if (ux_send->length > req->recv.rlength) {
			req->recv.mlength = req->recv.rlength;
			req->recv.rlength = ux_send->length;
		} else {
			req->recv.rlength = req->recv.mlength = ux_send->length;
		}

		memcpy(req->recv.recv_buf, oflow_va, req->recv.mlength);

		/* Drop reference to the overflow buffer.  It will be freed
		 * once all user data is copied out.
		 */
		oflow_buf_put(oflow_buf);

		/* Release the unexpected Put event record */
		free(ux_send);
	} else {
		if (event->hdr.event_type != C_EVENT_PUT) {
			CXIP_LOG_ERROR("Unexpected event type: %d\n",
					event->hdr.event_type);
			return;
		}

		CXIP_LOG_DBG("Matched in HW\n");

		req->recv.rlength = event->tgt_long.rlength;
		req->recv.mlength = event->tgt_long.mlength;
	}

	report_recv_completion(req);

	/* Free the user buffer request */
	cxip_cq_req_free(req);
}

static ssize_t cxip_trecv(struct fid_ep *ep, void *buf, size_t len, void *desc,
			  fi_addr_t src_addr, uint64_t tag, uint64_t ignore,
			  void *context)
{
	struct cxip_ep *cxi_ep;
	struct cxip_rx_ctx *rxc;
	struct cxip_domain *dom;
	int ret;
	struct cxi_iova recv_md;
	struct cxip_req *req;
	union c_cmdu cmd = {};

	if (!ep || !buf)
		return -FI_EINVAL;

	/* The input FID could be a standard endpoint (containing a RX
	 * context), or a RX context itself.
	 */
	switch (ep->fid.fclass) {
	case FI_CLASS_EP:
		cxi_ep = container_of(ep, struct cxip_ep, ep);
		rxc = cxi_ep->attr->rx_ctx;
		break;

	case FI_CLASS_RX_CTX:
		rxc = container_of(ep, struct cxip_rx_ctx, ctx);
		break;

	default:
		CXIP_LOG_ERROR("Invalid EP type\n");
		return -FI_EINVAL;
	}

	dom = rxc->domain;

	/* Map local buffer */
	ret = cxil_map(dom->dev_if->if_lni, (void *)buf, len,
		       CXI_MAP_PIN | CXI_MAP_NTA | CXI_MAP_WRITE, &recv_md);
	if (ret) {
		CXIP_LOG_DBG("Failed to map recv buffer: %d\n", ret);
		return ret;
	}

	/* Populate request */
	req = cxip_cq_req_alloc(rxc->comp.recv_cq, 1);
	if (!req) {
		CXIP_LOG_DBG("Failed to allocate request\n");
		ret = -FI_ENOMEM;
		goto unmap;
	}

	/* req->data_len, req->tag must be set later.  req->buf and req->data
	 * may be overwritten later.
	 */
	req->context = (uint64_t)context;
	req->flags = FI_TAGGED | FI_RECV;
	req->buf = 0;
	req->data = 0;
	req->cb = cxip_trecv_cb;

	req->recv.rxc = rxc;
	req->recv.recv_buf = buf;
	req->recv.recv_md = recv_md;
	req->recv.rlength = len;

	/* Build Append command descriptor */
	cmd.command.opcode     = C_CMD_TGT_APPEND;
	cmd.target.ptl_list    = C_PTL_LIST_PRIORITY;
	cmd.target.ptlte_index = rxc->rx_pte->pte->ptn;
	cmd.target.op_put      = 1;
	cmd.target.buffer_id   = req->req_id;
	cmd.target.lac         = recv_md.lac;
	cmd.target.start       = CXI_VA_TO_IOVA(&recv_md, buf);
	cmd.target.length      = len;
	cmd.target.use_once    = 1;
	cmd.target.match_bits  = tag;
	cmd.target.ignore_bits = ignore;

	fastlock_acquire(&rxc->lock);

	/* Issue Append command */
	ret = cxi_cq_emit_target(rxc->rx_cmdq, &cmd);
	if (ret) {
		CXIP_LOG_DBG("Failed to write Append command: %d\n", ret);

		/* Return error according to Domain Resource Management */
		ret = -FI_EAGAIN;
		goto unlock;
	}

	cxi_cq_ring(rxc->rx_cmdq);

	/* TODO take reference on EP or context for the outstanding request */
	fastlock_release(&rxc->lock);

	return FI_SUCCESS;

unlock:
	fastlock_release(&rxc->lock);
	cxip_cq_req_free(req);
unmap:
	cxil_unmap(dom->dev_if->if_lni, &recv_md);

	return ret;
}

static void cxip_tsend_cb(struct cxip_req *req, const union c_event *event)
{
	int ret;
	int event_rc;

	ret = cxil_unmap(req->cq->domain->dev_if->if_lni, &req->send.send_md);
	if (ret != FI_SUCCESS)
		CXIP_LOG_ERROR("Failed to free MD: %d\n", ret);

	event_rc = event->init_short.return_code;
	if (event_rc == C_RC_OK) {
		ret = req->cq->report_completion(req->cq, FI_ADDR_UNSPEC, req);
		if (ret != req->cq->cq_entry_size)
			CXIP_LOG_ERROR("Failed to report completion: %d\n",
				       ret);
	} else {
		ret = cxip_cq_report_error(req->cq, req, 0, FI_EIO, event_rc,
					   NULL, 0);
		if (ret != FI_SUCCESS)
			CXIP_LOG_ERROR("Failed to report error: %d\n", ret);
	}

	cxip_cq_req_free(req);
}

static ssize_t cxip_tsend(struct fid_ep *ep, const void *buf, size_t len,
			  void *desc, fi_addr_t dest_addr, uint64_t tag,
			  void *context)
{
	struct cxip_ep *cxi_ep;
	struct cxip_tx_ctx *txc;
	struct cxip_domain *dom;
	int ret;
	struct cxi_iova send_md;
	struct cxip_req *req;
	union c_cmdu cmd = {};
	struct cxip_addr caddr;
	union c_fab_addr dfa;
	uint32_t idx_ext;
	uint32_t pid_granule;
	uint32_t pid_idx;

	if (!ep || !buf)
		return -FI_EINVAL;

	/* The input FID could be a standard endpoint (containing a TX
	 * context), or a TX context itself.
	 */
	switch (ep->fid.fclass) {
	case FI_CLASS_EP:
		cxi_ep = container_of(ep, struct cxip_ep, ep);
		txc = cxi_ep->attr->tx_ctx;
		break;

	case FI_CLASS_TX_CTX:
		txc = container_of(ep, struct cxip_tx_ctx, fid.ctx);
		break;

	default:
		CXIP_LOG_ERROR("Invalid EP type\n");
		return -FI_EINVAL;
	}

	dom = txc->domain;

	/* Look up target CXI address */
	ret = _cxip_av_lookup(txc->av, dest_addr, &caddr);
	if (ret != FI_SUCCESS) {
		CXIP_LOG_DBG("Failed to look up FI addr: %d\n", ret);
		return ret;
	}

	/* Map local buffer */
	ret = cxil_map(dom->dev_if->if_lni, (void *)buf, len,
		       CXI_MAP_PIN | CXI_MAP_NTA | CXI_MAP_READ, &send_md);
	if (ret) {
		CXIP_LOG_DBG("Failed to map send buffer: %d\n", ret);
		return ret;
	}

	/* Populate request */
	req = cxip_cq_req_alloc(txc->comp.send_cq, 0);
	if (!req) {
		CXIP_LOG_DBG("Failed to allocate request\n");
		ret = -FI_ENOMEM;
		goto unmap;
	}

	req->context = (uint64_t)context;
	req->flags = FI_TAGGED | FI_SEND;
	req->data_len = 0;
	req->buf = 0;
	req->data = 0;
	req->tag = 0;

	req->cb = cxip_tsend_cb;
	req->send.send_md = send_md;

	/* Build Put command descriptor */
	pid_granule = dom->dev_if->if_pid_granule;
	pid_idx = CXIP_ADDR_RX_IDX(pid_granule, 0);
	cxi_build_dfa(caddr.nic, caddr.port, pid_granule, pid_idx, &dfa,
		      &idx_ext);

	cmd.full_dma.command.cmd_type = C_CMD_TYPE_DMA;
	cmd.full_dma.command.opcode = C_CMD_PUT;
	cmd.full_dma.index_ext = idx_ext;
	cmd.full_dma.lac = send_md.lac;
	cmd.full_dma.event_send_disable = 1;
	cmd.full_dma.restricted = 0;
	cmd.full_dma.dfa = dfa;
	cmd.full_dma.remote_offset = 0;
	cmd.full_dma.local_addr = CXI_VA_TO_IOVA(&send_md, buf);
	cmd.full_dma.request_len = len;
	cmd.full_dma.eq = txc->comp.send_cq->evtq->eqn;
	cmd.full_dma.user_ptr = (uint64_t)req;
	cmd.full_dma.match_bits = tag;

	fastlock_acquire(&txc->lock);

	/* Issue Put command */
	ret = cxi_cq_emit_dma(txc->tx_cmdq, &cmd.full_dma);
	if (ret) {
		CXIP_LOG_DBG("Failed to write DMA command: %d\n", ret);

		/* Return error according to Domain Resource Management */
		ret = -FI_EAGAIN;
		goto unlock;
	}

	cxi_cq_ring(txc->tx_cmdq);

	/* TODO take reference on EP or context for the outstanding request */
	fastlock_release(&txc->lock);

	return FI_SUCCESS;

unlock:
	fastlock_release(&txc->lock);
	cxip_cq_req_free(req);
unmap:
	cxil_unmap(dom->dev_if->if_lni, &send_md);

	return ret;
}

struct fi_ops_tagged cxip_ep_tagged_ops = {
	.size = sizeof(struct fi_ops_tagged),
	.recv = cxip_trecv,
	.recvv = fi_no_tagged_recvv,
	.recvmsg = fi_no_tagged_recvmsg,
	.send = cxip_tsend,
	.sendv = fi_no_tagged_sendv,
	.sendmsg = fi_no_tagged_sendmsg,
	.inject = fi_no_tagged_inject,
	.senddata = fi_no_tagged_senddata,
	.injectdata = fi_no_tagged_injectdata,
};

struct fi_ops_msg cxip_ep_msg_ops = {
	.size = sizeof(struct fi_ops_msg),
	.recv = fi_no_msg_recv,
	.recvv = fi_no_msg_recvv,
	.recvmsg = fi_no_msg_recvmsg,
	.send = fi_no_msg_send,
	.sendv = fi_no_msg_sendv,
	.sendmsg = fi_no_msg_sendmsg,
	.inject = fi_no_msg_inject,
	.senddata = fi_no_msg_senddata,
	.injectdata = fi_no_msg_injectdata,
};
